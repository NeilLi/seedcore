#!/usr/bin/env bash
# SeedCore development environment variables
# Usage: source ./env.host.sh

#!/bin/bash
# Setup script for database testing environment
# This script sets the correct environment variables for testing with port-forwarded databases

echo "Setting up database test environment..."

export PYTHONPATH=/home/ubuntu/project/seedcore/src

# Set database hosts to localhost (for port-forwarding)
export POSTGRES_HOST=localhost
export MYSQL_HOST=localhost
export NEO4J_HOST=localhost

# Update DSNs to use localhost
export PG_DSN=postgresql://postgres:password@localhost:5432/seedcore
export MYSQL_DSN=mysql+pymysql://seedcore:password@localhost:3306/seedcore
export NEO4J_URI=bolt://localhost:7687
### Postgres connection
export SEEDCORE_PG_DSN="postgresql://postgres:postgres@localhost:5432/seedcore"

# Keep other variables as they were
export POSTGRES_DB=postgres
export POSTGRES_USER=postgres
export POSTGRES_PASSWORD=password
export POSTGRES_PORT=5432

export MYSQL_DB=seedcore
export MYSQL_USER=seedcore
export MYSQL_PASSWORD=password
export MYSQL_PORT=3306

export NEO4J_USER=neo4j
export NEO4J_PASSWORD=password
export NEO4J_BOLT_PORT=7687
export NEO4J_HTTP_PORT=7474

echo "Environment variables set for localhost testing"
echo "Clearing cached database engines..."

# Clear cached engines to pick up new environment variables
python -c "
import sys
sys.path.insert(0, 'src')
from seedcore.database import get_sync_pg_engine, get_async_pg_engine, get_sync_mysql_engine, get_async_mysql_engine, get_neo4j_driver
get_sync_pg_engine.cache_clear()
get_async_pg_engine.cache_clear() 
get_sync_mysql_engine.cache_clear()
get_async_mysql_engine.cache_clear()
get_neo4j_driver.cache_clear()
print('Cleared all cached database engines')
"

echo ""
echo "Make sure your port-forwarding is active:"
echo "  kubectl port-forward svc/postgresql 5432:5432"
echo "  kubectl port-forward svc/mysql 3306:3306" 
echo "  kubectl port-forward svc/neo4j 7687:7687"
echo ""


### Ray cluster connection
export RAY_ADDRESS="ray://127.0.0.1:10001"
export RAY_NAMESPACE="seedcore-dev"

### Orchestrator (Serve app, internal)
export COORD_URL="http://127.0.0.1:8000/coordinator"
export COORD_PATHS="/tasks"

# Adjust if your gateway differs
# export COORD_URL="http://127.0.0.1:8000/coordinator"

# Orchestrator â†’ per-request budget for cognitive calls
export COGNITIVE_TIMEOUT_S=12
# Orchestrator client default; keep small but > COGNITIVE_TIMEOUT_S
export COORD_HTTP_TIMEOUT=15
# Make memory synthesis optional (or off)
export SEEDCORE_FACTS_ENABLED=false
export ENABLE_MEMORY_SYNTHESIS=false   # if you added this flag


### SeedCore API (FastAPI front door)
export SEEDCORE_API_URL="http://127.0.0.1:8002"
export SEEDCORE_API_TIMEOUT=5.0

### Dispatcher expectations
export EXPECT_DISPATCHERS=2
export EXPECT_GRAPH_DISPATCHERS=1
export STRICT_GRAPH_DISPATCHERS=false  # allow fewer than expected
export VERIFY_GRAPH_TASK=true

### OCPS & Cognitive escalation
export OCPS_DRIFT_THRESHOLD=0.5
export COGNITIVE_TIMEOUT_S=8.0
export COGNITIVE_MAX_INFLIGHT=64
export MAX_PLAN_STEPS=16

### Performance & SLOs
export FAST_PATH_LATENCY_SLO_MS=1000
export RAY_SERVE_MAX_QUEUE_LENGTH=2000
export RAY_SERVE_QUEUE_LENGTH_RESPONSE_DEADLINE_S=5.0

export OCPS_TAU_FAST=0.4
export OCPS_TAU_PLAN=0.8
export SEEDCORE_USE_SDK=true

export NIM_LLM_MODEL="meta/llama-3.1-8b-base"
export NIM_LLM_PROVIDER="nim"

export NIM_LLM_BASE_URL="http://a3055aa0ec20d4fefab34716edbe28ad-419314233.us-east-1.elb.amazonaws.com:8000/v1"
export NIM_LLM_API_KEY="none"

# === NIM: Retrieval / Embedding ===
export NIM_RETRIEVAL_MODEL="nvidia/nv-embedqa-e5-v5"
export NIM_RETRIEVAL_PROVIDER="nim"
export NIM_RETRIEVAL_BASE_URL="@http://af3a6a34f659a409584db07209d82853-1298671438.us-east-1.elb.amazonaws.com/v1"
export NIM_RETRIEVAL_API_KEY="none"

# =============================================================================
# LLM Configuration (OpenAI, Gemini, Claude, etc.)
# =============================================================================

# OpenAI API Key for GPT models
# Get your API key from: https://platform.openai.com/api-keys
export OPENAI_API_KEY="sk-proj-ZtChWrDgIzlJW..."

# LLM Provider Configuration
# Supported providers: openai, anthropic, google, azure, local, nim
# New multi-provider field (comma-separated). Falls back to LLM_PROVIDER if unset.
export LLM_PROVIDERS="nim"
# Back-compat single provider (deprecated)
export LLM_PROVIDER="openai"
# Deep profile provider is currently locked to OpenAI; other values are ignored at runtime.
export LLM_PROVIDER_DEEP="openai"

export VERIFY_NIM_TASK_EMBED=true

echo "[seedcore-dev] Environment variables set."

