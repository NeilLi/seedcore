#!/bin/bash

# Enhanced error handling for production
set -euo pipefail  # Exit on error, undefined vars, pipe failures

# Configuration
RAY_PORT=6379
DASHBOARD_PORT=8265
SERVE_PORT=8000
METRICS_PORT=8080
MAX_RETRIES=30
RETRY_DELAY=3

# Signal handling for graceful shutdown
cleanup() {
    echo "ğŸ›‘ Received shutdown signal, cleaning up..."
    if command -v ray &> /dev/null; then
        ray stop || true
    fi
    echo "âœ… Cleanup completed"
    exit 0
}

# Trap signals for graceful shutdown
trap cleanup SIGTERM SIGINT

echo "ğŸš€ Starting SeedCore Ray Head with ML Serve..."



# Clean up any existing Ray processes
echo "ğŸ§¹ Cleaning up any existing Ray processes..."
ray stop || true
pkill -f ray || true
sleep 2

# Start Ray head node with optimized configuration
echo "ğŸ”§ Starting Ray head node..."
ray start --head \
    --dashboard-host 0.0.0.0 \
    --dashboard-port ${DASHBOARD_PORT} \
    --port=${RAY_PORT} \
    --ray-client-server-port=10001 \
    --include-dashboard true \
    --metrics-export-port=${METRICS_PORT} \
    --num-cpus 1 \
    --temp-dir /tmp/ray \
    --log-style record

echo "â³ Starting Ray cluster..."

# Start Ray Serve with proper configuration
echo "ğŸš€ Starting Ray Serve..."
python -c "
import ray
from ray import serve
import os

# Initialize Ray connection - use 'auto' when running in same container
ray.init(address='auto', log_to_driver=False, namespace='serve')

# Start Serve with external access
serve.start(
    detached=True,
    http_options={
        'host': '0.0.0.0',
        'port': ${SERVE_PORT}
    }
)
print('âœ… Ray Serve started successfully')
"

# Deploy ML applications
echo "ğŸš€ Deploying ML applications..."
python /app/docker/serve_entrypoint.py

echo "â³ Deploying ML applications..."

# Display status and endpoints
echo ""
echo "ğŸ‰ SeedCore Ray Head with ML Serve is ready!"
echo "================================================"
echo "ğŸ“Š Ray Dashboard:     http://localhost:${DASHBOARD_PORT}"
echo "ğŸ”— ML Serve API:      http://localhost:${SERVE_PORT}"
echo "ğŸ“ˆ Metrics Export:    http://localhost:${METRICS_PORT}"
echo ""
echo "ğŸ¤– Available ML Endpoints:"
echo "   â€¢ Salience Scoring:    http://localhost:${SERVE_PORT}/ml/score/salience"
echo "   â€¢ Anomaly Detection:   http://localhost:${SERVE_PORT}/ml/detect/anomaly"
echo "   â€¢ Scaling Prediction:  http://localhost:${SERVE_PORT}/ml/predict/scaling"
echo "================================================"

# Keep the container running
echo "ğŸ”„ Ray head container is running..."
echo "ğŸ“Š Health checks are handled by Docker Compose"
echo "ğŸ” Monitor logs with: docker logs seedcore-ray-head -f"

# Keep the container alive
while true; do
    sleep 60
done 