# docker/serve_entrypoint.py
import ray, os, time, sys, traceback
from ray import serve
from src.seedcore.ml.serve_app import create_serve_app

RAY_ADDRESS = os.getenv("RAY_ADDRESS", "auto")
APP_NAME = "seedcore-ml"
ROUTE_PREFIX = "/ml"
MAX_DEPLOY_RETRIES = 30
DELAY = 2

def wait_for_http_ready(url, max_retries=30, delay=2):
    """Wait for HTTP endpoint to be ready."""
    import urllib.request
    for attempt in range(max_retries):
        try:
            with urllib.request.urlopen(url, timeout=5) as resp:
                if resp.status == 200:
                    print(f"âœ… Service ready at {url}")
                    return True
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"ðŸ”„ Service not ready ({e}); retrying in {delay}s...")
                time.sleep(delay)
            else:
                print(f"âŒ Service at {url} failed to become ready after {max_retries} attempts.")
    return False

def main():
    print(f"ðŸš€ Starting ML Serve deployment with Ray address: {RAY_ADDRESS}")
    
    while True:
        try:
            if not ray.is_initialized():
                print(f"ðŸ”§ Initializing Ray connection to {RAY_ADDRESS}...")
                ray.init(address=RAY_ADDRESS, log_to_driver=False, namespace="serve")
                print("âœ… Ray connection established")

            print("ðŸ”§ Creating ML Serve application...")
            app = create_serve_app()

            print("ðŸš€ Deploying ML application with route prefix /ml...")
            serve.run(app, name=APP_NAME, route_prefix=ROUTE_PREFIX)

            # Wait for the endpoint to be up
            print("â³ Waiting for deployment to be ready...")
            url = f"http://localhost:8000/ml/score/salience"
            if not wait_for_http_ready(url, MAX_DEPLOY_RETRIES, DELAY):
                raise RuntimeError("Deployed service endpoint did not respond.")

            print("ðŸŸ¢ ML Serve deployments are live!")
            print("ðŸ“Š Available endpoints:")
            print("   - Salience Scoring: /ml/score/salience")
            print("   - Anomaly Detection: /ml/detect/anomaly")
            print("   - Scaling Prediction: /ml/predict/scaling")
            print("   - Application ready at: http://localhost:8000/ml/")
            break

        except (ConnectionError, RuntimeError) as e:
            print(f"ðŸ”„ Ray or Serve not ready ({e}); retrying in 3s...")
            time.sleep(3)
        except Exception as e:
            print(f"âŒ Unexpected error during deployment: {e}")
            print(f"Error type: {type(e).__name__}")
            traceback.print_exc()
            time.sleep(3)

if __name__ == "__main__":
    main() 