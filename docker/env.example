# SeedCore Environment Configuration for Docker
# Copy this file to `.env` and edit values as needed for your environment.

# =============================================================================
# Database Configuration (Docker container names)
# =============================================================================

# --- PostgreSQL ---
POSTGRES_HOST=postgresql
POSTGRES_PORT=5432
POSTGRES_DB=postgres
POSTGRES_USER=postgres
POSTGRES_PASSWORD=password
PG_DSN=postgresql://postgres:password@postgresql:5432/seedcore

# --- MySQL ---
MYSQL_HOST=mysql
MYSQL_PORT=3306
MYSQL_DB=seedcore
MYSQL_USER=seedcore
MYSQL_PASSWORD=password
MYSQL_DSN=mysql+pymysql://seedcore:password@mysql:3306/seedcore

# --- Redis (auth disabled) ---
REDIS_HOST=redis-master
REDIS_PORT=6379
REDIS_DB=0
REDIS_URL=redis://redis-master:6379/0

# --- Neo4j (per your helm install) ---
NEO4J_HOST=neo4j
NEO4J_BOLT_PORT=7687
NEO4J_HTTP_PORT=7474
NEO4J_USER=neo4j
NEO4J_PASSWORD=password
NEO4J_BOLT_URL=bolt://neo4j:7687
NEO4J_HTTP_URL=http://neo4j:7474
NEO4J_URI=bolt://neo4j:7687
NEO4J_ENCRYPTED=false

# =============================================================================
# Energy Ledger Persistence
# =============================================================================

# Enable/disable the Energy Ledger persistence layer
# true|false
ENERGY_LEDGER_ENABLED=true

# Backend for the Energy Ledger store
# Supported: mysql (default), sql (alias of mysql), fs
ENERGY_LEDGER_BACKEND=mysql

# Root directory for FS backend (used only when ENERGY_LEDGER_BACKEND=fs)
# Example inside Docker volume: /data/seedcore/energy
ENERGY_LEDGER_ROOT=/tmp/seedcore

# =============================================================================
# Ray Configuration (Docker container setup)
# =============================================================================

# --- Ray (shared K8s defaults - safe for all pods) ---
# NOTE: Do NOT set RAY_ADDRESS here - it's configured per-pod role
RAY_HOST=seedcore-svc-head-svc
RAY_PORT=10001
RAY_SERVE_URL=http://seedcore-svc-stable-svc:8000
RAY_DASHBOARD_URL=http://seedcore-svc-head-svc:8265
RAY_NAMESPACE=seedcore-dev

# --- DGL (Deep Graph Library) Configuration ---
# DGL backend (pytorch, mxnet, tensorflow) - pytorch is recommended
DGLBACKEND=pytorch

# Number of GraphDispatcher instances to create during bootstrap
SEEDCORE_GRAPH_DISPATCHERS=1

# --- OCPS (Open Cognitive Processing System) Configuration ---
# Drift threshold for cognitive processing
OCPS_DRIFT_THRESHOLD=0.5

# Cognitive processing timeout in seconds
COGNITIVE_TIMEOUT_S=8.0

# Maximum number of cognitive requests in flight
COGNITIVE_MAX_INFLIGHT=64

# Fast path latency SLO in milliseconds
FAST_PATH_LATENCY_SLO_MS=1000

# Maximum number of plan steps
MAX_PLAN_STEPS=16

# --- PKG (Policy Knowledge Graph) Configuration ---
# Enable PKG evaluation in coordinator (1=enabled, 0=disabled)
COORDINATOR_PKG_ENABLED=1

# Path to PKG WASM binary file
PKG_WASM_PATH=/app/data/opt/pkg/policy_rules.wasm

# PKG snapshot version for policy rules and ontology
PKG_SNAPSHOT_VERSION=rules@1.3.0+ontology@0.9.1

# PKG evaluation timeout in milliseconds
PKG_EVAL_TIMEOUT_MS=10

# =============================================================================
# API Configuration
# =============================================================================

# The API root URL as seen from inside Docker (usually for other containers)
# Note: Use SEEDCORE_API_ADDRESS for internal container communication

# API server bind host and port
API_HOST=0.0.0.0
API_PORT=8002

# =============================================================================
# Development Configuration
# =============================================================================

# Set DEV_RELOAD=1 to enable hot-reload during local development
# Set to 0 or leave unset for production/stable operation
DEV_RELOAD=0

# =============================================================================
# Memory and Working Memory Configuration
# =============================================================================

# Enable/disable MwManager integration (1=enabled, 0=disabled)
MW_ENABLED=1

# Memory Working (MW) actor name
MW_ACTOR_NAME=mw

# Shared cache name for memory operations
SHARED_CACHE_NAME=shared_cache

# Auto-create resources (0=disabled, 1=enabled)
AUTO_CREATE=0

# Lookup retry configuration
LOOKUP_RETRIES=12
LOOKUP_BACKOFF_S=0.5
LOOKUP_BACKOFF_MULT=1.5

# =============================================================================
# Logging
# =============================================================================

# Logging level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# =============================================================================
# Environment
# =============================================================================

# Set to "development" or "production"
ENVIRONMENT=development

# =============================================================================
# Ray Dashboard Configuration
# =============================================================================

# Public Grafana URL for Ray dashboard (used in browser iframes)
# Use http://localhost:3000 for local dev, or http://YOUR_PUBLIC_IP:3000 in production
PUBLIC_GRAFANA_URL=http://localhost:3000


# =============================================================================
# Optional External Services (uncomment and set as needed)
# =============================================================================

# RAGFLOW_API_URL=http://localhost:9380
# CHROMA_HOST=localhost
# CHROMA_PORT=8000


# =============================================================================
# LLM Configuration (OpenAI, Gemini, Claude, etc.)
# =============================================================================

# OpenAI API Key for GPT models
# Get your API key from: https://platform.openai.com/api-keys
# IMPORTANT: Never commit actual API keys to version control
OPENAI_API_KEY=sk-proj-YOUR_API_KEY_HERE

# LLM Provider Configuration
# Supported providers: openai, anthropic, google, azure, local, nim
# New multi-provider field (comma-separated). Falls back to LLM_PROVIDER if unset.
LLM_PROVIDERS=openai,anthropic,nim
# Back-compat single provider (deprecated)
LLM_PROVIDER=openai
# Deep profile provider is currently locked to OpenAI; other values are ignored at runtime.
LLM_PROVIDER_DEEP=

# LLM Model Configuration
LLM_MODEL=gpt-4o
LLM_MAX_TOKENS=1024
LLM_TEMPERATURE=0.7
LLM_TOP_P=1.0
LLM_FREQUENCY_PENALTY=0.0
LLM_PRESENCE_PENALTY=0.0

# LLM API Configuration
LLM_API_BASE=
LLM_API_VERSION=
LLM_TIMEOUT=60
LLM_MAX_RETRIES=3
LLM_RETRY_DELAY=1.0

# --- OpenAI ---
# OPENAI_API_KEY already defined above
OPENAI_MODEL=gpt-4o

# --- Anthropic ---
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL=claude-3-5-sonnet

# --- Google Vertex (Gemini) ---
GOOGLE_VERTEX_MODEL=gemini-1.5-pro

# --- Azure OpenAI ---
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_DEPLOYMENT=

# --- NIM / Local ---
# === NIM: LLaMA (Generation) ===
NIM_LLM_ENDPOINT=http://seedcore-nim-llama-svc:8080/v1
NIM_LLM_MODEL=meta/llama-3.1-8b-base
NIM_LLM_PROVIDER=nim

NIM_LLM_BASE_URL=@http://a3055aa0ec20d4fefab34716edbe28ad-419314233.us-east-1.elb.amazonaws.com:8000/v1
NIM_LLM_API_KEY=none

# === NIM: Retrieval / Embedding ===
NIM_RETRIEVAL_ENDPOINT=http://seedcore-nim-retrieval-svc:8080/v1
NIM_RETRIEVAL_MODEL=nvidia/nv-embedqa-e5-v5
NIM_RETRIEVAL_PROVIDER=nim

NIM_RETRIEVAL_BASE_URL=@http://af3a6a34f659a409584db07209d82853-1298671438.us-east-1.elb.amazonaws.com/v1
NIM_RETRIEVAL_API_KEY=none


# === Shared ===
SEEDCORE_NIM_TIMEOUT_S=30

# Excellent â€” this is exactly the dual-NIM setup the SeedCore agentic fabric needs:
# nim-llama (text generation / reasoning model: meta/llama-3.1-8b-base:1.1.2)
# nim-retrieval (embedding & retrieval model: nvidia/nv-embedqa-e5-v5:1)

# Embeddings configuration for NIM
EMBED_PROVIDER=nim
SEEDCORE_NIM_EMBED_MODEL=meta/llama-3.1-embedding
# choose payload mode if loader can supply text; else feature mode
NIM_INPUT_MODE=payload
# optional dual-write to NIM vector index
NIM_UPSERT=1

# LLM Streaming Configuration
LLM_STREAM=false
LLM_STREAM_CHUNK_SIZE=1

# LLM Safety and Content Filtering
LLM_SAFE_MODE=false
LLM_CONTENT_FILTER=true

# LLM Rate Limiting
LLM_RATE_LIMIT_REQUESTS_PER_MINUTE=60
LLM_RATE_LIMIT_TOKENS_PER_MINUTE=150000

# LLM Caching Configuration
LLM_ENABLE_CACHING=true
LLM_CACHE_TTL=3600

# LLM Logging Configuration
LLM_LOG_REQUESTS=true
LLM_LOG_RESPONSES=false
LLM_LOG_TOKENS=true

# LLM Advanced Configuration
LLM_SYSTEM_PROMPT=
LLM_USER_PROMPT_TEMPLATE=

# Seedcore Cloud Native
SEEDCORE_NS=seedcore-dev
SEEDCORE_STAGE=dev
COG_APP_NAME=seedcore-dev-dev-cognitive_core
COG_MIN_READY=1
SEEDCORE_API_ADDRESS=seedcore-api:8002

# =============================================================================
# Serve Starting configuration
# Until workers (not just head) join, set (for example):
# =============================================================================

RAY_MIN_NODES=2
RAY_MIN_CPUS=2

# =============================================================================
# Serve Starting configuration
# to avoid CPU oversubscription in math libs during training
# =============================================================================
OMP_NUM_THREADS=1
MKL_NUM_THREADS=1
TOKENIZERS_PARALLELISM=false

# =============================================================================
# Boostrap configuration
# =============================================================================
SEEDCORE_BOOTSTRAP_OPTIONAL=1


