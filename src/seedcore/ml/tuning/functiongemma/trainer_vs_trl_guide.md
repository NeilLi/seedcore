# Trainer vs TRL SFTTrainer: FunctionGemma Training Guide

Great question â€” this is exactly the right moment to compare them, because **your current LoRA setup is already correct**, and the choice between **plain `Trainer` vs TRL `SFTTrainer`** determines *how far* you can push FunctionGemma.

I'll break this down **precisely in the context of your SeedCore intent compiler**, not generic LLM training.

---

## TL;DR (Executive Summary)

| Aspect                               | Your current LoRA + Trainer | TRL `SFTTrainer` |
| ------------------------------------ | --------------------------- | ---------------- |
| Works for FunctionGemma tool-calling | âœ… Yes                       | âœ… Yes            |
| Simplicity                           | â­â­â­â­â­                       | â­â­â­              |
| Control over format                  | â­â­â­â­                        | â­â­â­â­â­            |
| Multi-turn / tool-call robustness    | â­â­â­                         | â­â­â­â­â­            |
| Preference learning (later)          | âŒ No                        | âœ… Yes            |
| Long-term roadmap                    | âš ï¸ Limited                  | ğŸš€ Best          |

**Verdict:**
ğŸ‘‰ **You did the right thing starting with plain LoRA + Trainer**
ğŸ‘‰ **Move to TRL only after the first LoRA proves value**

---

## 1ï¸âƒ£ What you implemented (baseline LoRA)

You currently have:

```python
Trainer(
    model=model,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
)
```

### What this gives you

âœ… Deterministic supervised learning
âœ… Very stable on CPU
âœ… Minimal dependencies
âœ… Easy to debug
âœ… Perfect for **Phase 1: "make the model stop being dumb"**

### What it learns well

* Mapping:

  ```
  user text â†’ function name
  ```
* Emitting *some* JSON-like structure
* Replacing fallback heuristics

### What it does **not** learn well

* Strict JSON validity under stress
* Tool-call schema discipline
* Multi-tool ambiguity resolution
* Confidence calibration
* "Don't hallucinate tools"

That's why you still see:

```
function = "unknown"
confidence = 0.2
```

Even when the model *kind of* understands.

---

## 2ï¸âƒ£ What TRL `SFTTrainer` changes (important)

TRL (`trl` library) was built **specifically for instruction + tool tuning**.

### Key difference (very important)

> **TRL trains on *conversations*, not raw token sequences**

That matters because **FunctionGemma is instruction-tuned**, not a base LM.

---

### Your current training signal (simplified)

```
[USER TEXT] â†’ [ASSISTANT TOKENS]
```

### TRL training signal

```
(system)
(user)
(assistant â†’ tool_call JSON)
```

That aligns *exactly* with:

```json
{
  "messages": [...],
  "tools": [...]
}
```

Which you already prepared ğŸ‘

---

## 3ï¸âƒ£ Concrete comparison in your codebase

### Your current trainer

```python
Trainer(
    model=model,
    args=args,
    train_dataset=dataset["train"],
)
```

### TRL version (drop-in replacement)

```python
from trl import SFTTrainer

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=False,
)
```

**That's it.**
No architecture change. Same LoRA. Same dataset.

---

## 4ï¸âƒ£ Why TRL is better *for intent compilers*

### 1. Tool-call correctness

TRL **reinforces the tool-call boundary**, not just text.

That means fewer cases like:

```json
{"function": "unknown"}
```

when a schema exists.

---

### 2. Schema faithfulness

TRL implicitly teaches:

> "If tools exist â†’ choose one"

Plain Trainer does not.

---

### 3. Future-proofing (this matters for SeedCore)

Once you move beyond fallback imitation, TRL unlocks:

| Feature                 | Why it matters                  |
| ----------------------- | ------------------------------- |
| Preference tuning (DPO) | Train confidence calibration    |
| Rejection sampling      | Reduce hallucinations           |
| Multi-tool arbitration  | Critical for hotels / buildings |
| Reward modeling         | Energy-aware routing            |

You *cannot* do this with plain Trainer.

---

## 5ï¸âƒ£ When NOT to use TRL (important)

Do **NOT** use TRL if:

âŒ You are still debugging data format
âŒ You don't yet trust your fallback outputs
âŒ You are CPU-only and want fastest iteration
âŒ You're still under 1k samples

ğŸ‘‰ That's why your current choice was correct.

---

## 6ï¸âƒ£ Recommended migration path (very important)

### Phase 1 (you just finished)

âœ” Plain Trainer + LoRA
âœ” Validate pipeline
âœ” Fix intent compiler bugs
âœ” Ensure model loads + routes

### Phase 2 (next step)

â¡ Switch Trainer â†’ `SFTTrainer`
â¡ Keep **same dataset**
â¡ Train again (2â€“3 epochs)

### Phase 3 (later)

â¡ Add **negative samples**
â¡ Add **ambiguous intents**
â¡ Add **confidence targets**
â¡ Move to **DPO**

---

## 7ï¸âƒ£ Minimal TRL upgrade for you (ready-to-use)

Create a second trainer file:

```python
# train_intent_lora_trl.py
from trl import SFTTrainer

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    max_seq_length=2048,
    packing=False,
)

trainer.train()
```

Everything else stays the same.

---

## 8ï¸âƒ£ Final recommendation (clear & honest)

> â Your intuition was correct:
> **The model "doesn't work well" because it's not trained for tool discipline â€” not because FunctionGemma is weak.** â

FunctionGemma **needs**:

* structured supervision
* conversation-aware loss
* schema pressure

TRL gives you that â€” **when you're ready**.

---

If you want, next we can:

* design **negative samples**
* design **confidence targets**
* or build a **hotel-specific intent dataset**

Just say **"next: dataset strategy"** or **"next: TRL migration"** ğŸš€

