# Copyright 2024 SeedCore Contributors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This file is auto-generated by create_seedcore_skeleton.py
"""
Bounded energy weight containers and simple adaptation utilities.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict
import numpy as np


@dataclass
class EnergyWeights:
    """Container for energy weights with projection keeping maps non-expansive.

    - W_pair: symmetric pairwise collaboration weights [N, N]
    - W_hyper: hyperedge pattern weights [E]
    - alpha_entropy: scalar α for role entropy
    - lambda_reg: scalar λ for L2 regularizer
    - beta_mem: scalar β for memory cost
    """

    W_pair: np.ndarray  # shape [N, N]
    W_hyper: np.ndarray  # shape [E]
    alpha_entropy: float
    lambda_reg: float
    beta_mem: float

    def project(self, r_pair: float = 1.0, r_hyper: float = 1.0) -> None:
        """Clip spectral norms / magnitudes to keep maps non-expansive."""
        # Symmetrize and clip spectral norm for W_pair
        if self.W_pair.size > 0:
            self.W_pair = 0.5 * (self.W_pair + self.W_pair.T)
            spectral_norm = float(np.linalg.norm(self.W_pair, 2))
            if spectral_norm > r_pair and spectral_norm > 1e-8:
                self.W_pair *= (r_pair / spectral_norm)
        # Clip element-wise for hyperedge weights
        if self.W_hyper.size > 0:
            self.W_hyper = np.clip(self.W_hyper, -r_hyper, r_hyper)

    def as_dict(self) -> Dict[str, float]:
        # Scalars only (avoid serializing arrays)
        return {
            "alpha_entropy": float(self.alpha_entropy),
            "lambda_reg": float(self.lambda_reg),
            "beta_mem": float(self.beta_mem),
        }


def adapt_energy_weights(
    weights: EnergyWeights,
    dspec: float,
    dacc: float,
    dsmart: float,
    dreason: float,
    gamma_spec: float = 0.1,
    gamma_acc: float = 0.1,
    gamma_smart: float = 0.1,
    gamma_reason: float = 0.1,
) -> None:
    """Simple multiplicative adaptation followed by projection.

    This is a placeholder that nudges scalars and then re-projects matrices.
    """
    # Scalars
    weights.alpha_entropy *= (1.0 - gamma_spec * dspec + max(0.0, -dreason) * gamma_reason)
    weights.lambda_reg *= (1.0 + gamma_smart * dsmart)
    weights.beta_mem *= (1.0 + gamma_smart * dsmart)
    # Gentle hyper weight nudge
    if weights.W_hyper.size > 0:
        weights.W_hyper *= (1.0 + gamma_reason * dreason)
    # Pair weights: nudge uniformly based on spec/acc
    if weights.W_pair.size > 0:
        weights.W_pair *= (1.0 + gamma_spec * dspec + gamma_acc * dacc)
    # Re-project to bounds
    weights.project()
