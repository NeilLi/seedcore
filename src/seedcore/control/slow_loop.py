# Copyright 2024 SeedCore Contributors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This file is auto-generated by create_seedcore_skeleton.py
"""
Slow Loop (~2s): role evolution / PSO (Section 6, 3.x.4).
"""

import random
from typing import List, Union
from ..organs.base import Organ
from ..agents.base import Agent
from ..ops.energy.api import _ledger
from ..ops.energy.grad_adapter import get_global_gradient_bus
from ..ops.energy.calculator import role_entropy_grad
from ..ops.state.state_aggregator import StateAggregator  # if available during runtime

def slow_loop_update_roles(organs_or_agents: Union[List[Organ], List[Agent]], learning_rate: float = 0.05):
    """
    PSO or hillclimb to adjust role distributions based on performance.
    This function iterates through organs/agents and adjusts agent role probabilities
    based on energy performance and task success rates.
    
    Args:
        organs_or_agents: Either a list of organs or a list of agents
        learning_rate: Rate at which roles are adjusted
    """
    # Determine if we received organs or agents
    if organs_or_agents and hasattr(organs_or_agents[0], 'agents'):
        # We received organs, extract all agents
        all_agents = [agent for organ in organs_or_agents for agent in organ.agents.values()]
        print(f"Running slow loop for {len(organs_or_agents)} organs with {len(all_agents)} agents...")
    else:
        # We received agents directly
        all_agents = organs_or_agents
        print(f"Running slow loop for {len(all_agents)} agents...")
    
    try:
        # Try gradient-driven update (preferred)
        # Build a minimal unified state snapshot if aggregator available
        try:
            from ..organs.organism_manager import organism_manager
            if organism_manager is not None:
                import asyncio
                us = asyncio.run(organism_manager.get_unified_state())
            else:
                us = None
        except Exception:
            us = None
        bus = get_global_gradient_bus()
        grads = bus.latest(us if us is not None else {"agents": {}}, allow_stale=True)
        # Construct P matrix from current agents
        P = []
        for agent in all_agents:
            if getattr(agent, "role_probs", None):
                P.append([
                    float(agent.role_probs.get('E', 0.0)),
                    float(agent.role_probs.get('S', 0.0)),
                    float(agent.role_probs.get('O', 0.0)),
                ])
        import numpy as np
        P = np.asarray(P, dtype=np.float32) if P else np.zeros((0, 3), dtype=np.float32)
        if P.size > 0:
            dP_entropy = role_entropy_grad(P) * float(grads.dE_dP_entropy)
            # Apply a small step to role probs along -gradient (descent)
            step = float(learning_rate)
            for idx, agent in enumerate(all_agents):
                if getattr(agent, "role_probs", None) and idx < dP_entropy.shape[0]:
                    agent.role_probs['E'] = max(0.0, min(1.0, float(agent.role_probs.get('E', 0.0) - step * dP_entropy[idx, 0])))
                    agent.role_probs['S'] = max(0.0, min(1.0, float(agent.role_probs.get('S', 0.0) - step * dP_entropy[idx, 1])))
                    agent.role_probs['O'] = max(0.0, min(1.0, float(agent.role_probs.get('O', 0.0) - step * dP_entropy[idx, 2])))
                    # Renormalize
                    s = (agent.role_probs['E'] + agent.role_probs['S'] + agent.role_probs['O']) or 1.0
                    agent.role_probs['E'] /= s
                    agent.role_probs['S'] /= s
                    agent.role_probs['O'] /= s
        print("Gradient-driven role update applied (with fallback if partial)")
    except Exception:
        # Fallback: legacy heuristic on ledger totals
        for agent in all_agents:
            current_energy = _ledger.total
            energy_efficiency = 1.0 / (1.0 + abs(current_energy))
            if not agent.role_probs:
                continue
            if current_energy > 5.0:
                target_role = 'E'
            elif current_energy < -5.0:
                target_role = 'S'
            else:
                target_role = 'O'
            if target_role in agent.role_probs:
                increase = (1.0 - agent.role_probs[target_role]) * learning_rate
                agent.role_probs[target_role] += increase
                total_decrease = increase
                other_roles = [r for r in agent.role_probs if r != target_role]
                current_sum_others = sum(agent.role_probs[r] for r in other_roles)
                if current_sum_others > 0:
                    for role in other_roles:
                        agent.role_probs[role] -= total_decrease * (agent.role_probs[role] / current_sum_others)
            total_prob = sum(agent.role_probs.values())
            for role in agent.role_probs:
                agent.role_probs[role] /= total_prob
            agent.capability = min(1.0, max(0.0, agent.capability + energy_efficiency * 0.1))
            print(f"Updated agent {agent.agent_id} roles (heuristic): {agent.role_probs}, capability: {agent.capability:.3f}")

    print("Slow loop finished.")

def slow_loop_update_roles_simple(agents: List[Agent], learning_rate: float = 0.05):
    """
    Simple heuristic-based role adjustment that strengthens dominant roles.
    This is an alternative implementation that doesn't require energy context.
    """
    print(f"Running simple slow loop for {len(agents)} agents...")
    
    for agent in agents:
        # Find the current dominant role
        if not agent.role_probs:
            continue
        
        dominant_role = max(agent.role_probs, key=agent.role_probs.get)
        
        # Increase the dominant role's probability
        increase = (1.0 - agent.role_probs[dominant_role]) * learning_rate
        agent.role_probs[dominant_role] += increase
        
        # Decrease other roles' probabilities proportionally
        total_decrease = increase
        other_roles = [r for r in agent.role_probs if r != dominant_role]
        
        # Normalize the other roles so they sum to the total decrease
        current_sum_others = sum(agent.role_probs[r] for r in other_roles)
        if current_sum_others > 0:
            for role in other_roles:
                agent.role_probs[role] -= total_decrease * (agent.role_probs[role] / current_sum_others)

        # Ensure probabilities still sum to 1 (correcting for float inaccuracies)
        total_prob = sum(agent.role_probs.values())
        for role in agent.role_probs:
            agent.role_probs[role] /= total_prob

    print("Simple slow loop finished.")

def get_role_performance_metrics(organs: List[Organ]) -> dict:
    """
    Calculate performance metrics for each role type across all agents.
    """
    role_metrics = {'E': 0, 'S': 0, 'O': 0}
    role_counts = {'E': 0, 'S': 0, 'O': 0}
    
    for organ in organs:
        for agent in organ.agents.values():
            for role, prob in agent.role_probs.items():
                role_metrics[role] += agent.capability * prob
                role_counts[role] += 1
    
    # Average performance per role
    for role in role_metrics:
        if role_counts[role] > 0:
            role_metrics[role] /= role_counts[role]
    
    return role_metrics
