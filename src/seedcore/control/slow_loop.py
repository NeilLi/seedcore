# This file is auto-generated by create_seedcore_skeleton.py
"""
Slow Loop (~2s): role evolution / PSO (Section 6, 3.x.4).
"""

import random
from typing import List, Union
from ..organs.base import Organ
from ..agents.base import Agent
from ..energy.api import _ledger

def slow_loop_update_roles(organs_or_agents: Union[List[Organ], List[Agent]], learning_rate: float = 0.05):
    """
    PSO or hillclimb to adjust role distributions based on performance.
    This function iterates through organs/agents and adjusts agent role probabilities
    based on energy performance and task success rates.
    
    Args:
        organs_or_agents: Either a list of organs or a list of agents
        learning_rate: Rate at which roles are adjusted
    """
    # Determine if we received organs or agents
    if organs_or_agents and isinstance(organs_or_agents[0], Organ):
        # We received organs, extract all agents
        all_agents = [agent for organ in organs_or_agents for agent in organ.agents]
        print(f"Running slow loop for {len(organs_or_agents)} organs with {len(all_agents)} agents...")
    else:
        # We received agents directly
        all_agents = organs_or_agents
        print(f"Running slow loop for {len(all_agents)} agents...")
    
    for agent in all_agents:
        # Calculate performance metric based on energy efficiency
        current_energy = _ledger.total
        energy_efficiency = 1.0 / (1.0 + abs(current_energy))  # Higher efficiency when energy is closer to 0
        
        # Find the current dominant role
        if not agent.role_probs:
            continue
        
        dominant_role = max(agent.role_probs, key=agent.role_probs.get)
        
        # Energy-aware role adjustment
        if current_energy > 5.0:  # High energy state - need exploration
            target_role = 'E'
        elif current_energy < -5.0:  # Low energy state - need specialization
            target_role = 'S'
        else:  # Moderate energy state - need optimization
            target_role = 'O'
        
        # Increase the target role's probability based on energy state
        if target_role in agent.role_probs:
            increase = (1.0 - agent.role_probs[target_role]) * learning_rate
            agent.role_probs[target_role] += increase
            
            # Decrease other roles' probabilities proportionally
            total_decrease = increase
            other_roles = [r for r in agent.role_probs if r != target_role]
            
            # Normalize the other roles so they sum to the total decrease
            current_sum_others = sum(agent.role_probs[r] for r in other_roles)
            if current_sum_others > 0:
                for role in other_roles:
                    agent.role_probs[role] -= total_decrease * (agent.role_probs[role] / current_sum_others)
        
        # Ensure probabilities still sum to 1 (correcting for float inaccuracies)
        total_prob = sum(agent.role_probs.values())
        for role in agent.role_probs:
            agent.role_probs[role] /= total_prob
        
        # Update capability based on energy efficiency
        agent.capability = min(1.0, max(0.0, agent.capability + energy_efficiency * 0.1))
        
        print(f"Updated agent {agent.agent_id} roles: {agent.role_probs}, capability: {agent.capability:.3f}")

    print("Slow loop finished.")

def slow_loop_update_roles_simple(agents: List[Agent], learning_rate: float = 0.05):
    """
    Simple heuristic-based role adjustment that strengthens dominant roles.
    This is an alternative implementation that doesn't require energy context.
    """
    print(f"Running simple slow loop for {len(agents)} agents...")
    
    for agent in agents:
        # Find the current dominant role
        if not agent.role_probs:
            continue
        
        dominant_role = max(agent.role_probs, key=agent.role_probs.get)
        
        # Increase the dominant role's probability
        increase = (1.0 - agent.role_probs[dominant_role]) * learning_rate
        agent.role_probs[dominant_role] += increase
        
        # Decrease other roles' probabilities proportionally
        total_decrease = increase
        other_roles = [r for r in agent.role_probs if r != dominant_role]
        
        # Normalize the other roles so they sum to the total decrease
        current_sum_others = sum(agent.role_probs[r] for r in other_roles)
        if current_sum_others > 0:
            for role in other_roles:
                agent.role_probs[role] -= total_decrease * (agent.role_probs[role] / current_sum_others)

        # Ensure probabilities still sum to 1 (correcting for float inaccuracies)
        total_prob = sum(agent.role_probs.values())
        for role in agent.role_probs:
            agent.role_probs[role] /= total_prob

    print("Simple slow loop finished.")

def get_role_performance_metrics(organs: List[Organ]) -> dict:
    """
    Calculate performance metrics for each role type across all agents.
    """
    role_metrics = {'E': 0, 'S': 0, 'O': 0}
    role_counts = {'E': 0, 'S': 0, 'O': 0}
    
    for organ in organs:
        for agent in organ.agents:
            for role, prob in agent.role_probs.items():
                role_metrics[role] += agent.capability * prob
                role_counts[role] += 1
    
    # Average performance per role
    for role in role_metrics:
        if role_counts[role] > 0:
            role_metrics[role] /= role_counts[role]
    
    return role_metrics
