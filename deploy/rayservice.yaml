apiVersion: ray.io/v1
kind: RayService
metadata:
  name: seedcore-svc
  namespace: seedcore-dev
spec:
  # 1. Service Unhealthy Second Threshold (Critical for High Availability)
  # If the head node dies, KubeRay waits this long before replacing it.
  serviceUnhealthySecondThreshold: 300 
  deploymentUnhealthySecondThreshold: 300
  # ===================================================================
  #  Serve Configuration (Optimized for Ray 2.33 + Py3.11)
  # ===================================================================
  serveConfigV2: |
    http_options:
      host: 0.0.0.0
      port: 8000
      location: HeadOnly  # Proxy stays on head (Ingress Controller target)

    applications:
      # ----------------------------------------------------------------
      # 1. ML Service (Compute Heavy)
      # ----------------------------------------------------------------
      - name: ml_service
        import_path: entrypoints.ml_entrypoint:build_ml_service
        route_prefix: /ml
        args:
          env: "production"
        deployments:
          - name: MLService
            # AUTOSCALING: CPU intensive, scale based on load
            autoscaling_config:
              min_replicas: 1
              max_replicas: 5
              target_num_ongoing_requests_per_replica: 10
              downscale_delay_s: 300
            max_concurrent_queries: 20
            ray_actor_options:
              num_cpus: 0.5        # Give a full core for numpy/torch logic
              memory: 2147483648   # 2GB
              # REMOVED: {"head_node": 0.001} -> Let this run on workers!

      # ----------------------------------------------------------------
      # 2. Cognitive Service (I/O Heavy - LLM Calls)
      # ----------------------------------------------------------------
      - name: cognitive
        import_path: entrypoints.cognitive_entrypoint:build_cognitive_app
        route_prefix: /cognitive
        args:
          env: "production"
        deployments:
          - name: CognitiveService
            # AUTOSCALING: Scale on I/O wait
            autoscaling_config:
              min_replicas: 1
              max_replicas: 10
              target_num_ongoing_requests_per_replica: 50
            max_concurrent_queries: 100  # High concurrency (Async I/O)
            ray_actor_options:
              num_cpus: 0.2        # Low CPU usage (waiting for HTTP)
              memory: 2147483648   # 2GB

      # ----------------------------------------------------------------
      # 3. Coordinator (Orchestrator - Singleton)
      # ----------------------------------------------------------------
      - name: coordinator
        import_path: entrypoints.coordinator_entrypoint:build_coordinator
        route_prefix: /pipeline
        deployments:
          - name: Coordinator
            num_replicas: 1        # Singleton (Manages state/sequences)
            max_concurrent_queries: 200
            ray_actor_options:
              num_cpus: 0.5
              resources: {"head_node": 0.001} # Keep near Proxy if latency sensitive

      # ----------------------------------------------------------------
      # 4. Organism (The Router - Critical Path)
      # ----------------------------------------------------------------
      - name: organism
        import_path: entrypoints.organism_entrypoint:build_organism_app
        route_prefix: /organism
        args:
          env: "production"
        deployments:
          - name: OrganismService
            # AUTOSCALING: Vital for routing throughput
            autoscaling_config:
              min_replicas: 1
              max_replicas: 5
              target_num_ongoing_requests_per_replica: 100
            max_concurrent_queries: 200 # Matches our Async/Lazy optimization
            health_check_period_s: 10
            health_check_timeout_s: 30
            ray_actor_options:
              num_cpus: 0.5         # Needs CPU for routing logic/hashing
              memory: 2147483648    # 2GB

      # ----------------------------------------------------------------
      # 5. Ops & Shared Utilities
      # ----------------------------------------------------------------
      - name: ops
        import_path: entrypoints.ops_entrypoint:build_ops_app
        route_prefix: /ops
        deployments:
          - name: OpsGateway
            num_replicas: 1
            ray_actor_options:
              num_cpus: 0.1
              resources: {"head_node": 0.001} # Pin ingress-like services

          - name: EventizerService
            autoscaling_config:
              min_replicas: 1
              max_replicas: 5
              target_num_ongoing_requests_per_replica: 50
            max_concurrent_queries: 100
            ray_actor_options:
              num_cpus: 0.2
              memory: 2147483648

          # --- Singletons (Stateful) ---
          - name: FactManagerService
            num_replicas: 1
            ray_actor_options:
              num_cpus: 0.1
              memory: 1073741824 # 1GB

          - name: StateService
            num_replicas: 1
            ray_actor_options:
              num_cpus: 0.2
              memory: 1073741824

          - name: EnergyService
            num_replicas: 1
            ray_actor_options:
              num_cpus: 0.1
              memory: 1073741824

      # ----------------------------------------------------------------
      # 6. MCP Protocol
      # ----------------------------------------------------------------
      - name: mcp
        import_path: entrypoints.mcp_entrypoint:build_mcp_app
        route_prefix: /mcp
        deployments:
          - name: MCPService
            num_replicas: 1
            max_concurrent_queries: 50
            ray_actor_options:
              num_cpus: 0.1
              memory: 1073741824

  # ===================================================================
  #  Ray Cluster Configuration
  # ===================================================================
  rayClusterConfig:
    rayVersion: "2.33.0"
    # -------------------------------------------------------------------
    #  Head Node Specification
    # -------------------------------------------------------------------
    headGroupSpec:
      # The Ray Operator will automatically create a Service for the head node.
      # It will also create the 'seedcore-svc-serve-svc' based on the serveConfigV2.
      # By setting proxy_location to HeadOnly, the operator correctly configures
      # the serve service to target ONLY the head pod.
      serviceType: ClusterIP
      rayStartParams:
        dashboard-host: "0.0.0.0"
        dashboard-port: "8265"
        ray-client-server-port: "10001"
        num-cpus: "2"

      template:
        spec:
          containers:
            - name: ray-head
              image: seedcore:latest
              imagePullPolicy: IfNotPresent
              envFrom:
                - secretRef: { name: seedcore-env-secret }
              env:
                - name: POD_NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
                - name: SERVE_SERVICE
                  value: "stable"
                - name: ENABLE_ML_SERVICE
                  value: "true" # Set to "false" to disable ML service
                - name: DGL_GRAPHBOLT_SKIP
                  value: "0"
                - name: RAY_OVERRIDE_RESOURCES
                  value: '{"head_node": 1}'
                - name: RAY_NAMESPACE
                  value: "seedcore-dev"
                - name: SEEDCORE_NS
                  value: "seedcore-dev"
                - name: XGB_STORAGE_PATH
                  value: "/app/data/models"
                # ✅ Routing environment variables for coordinator access
                - name: OCPS_DRIFT_THRESHOLD
                  value: "0.5"
                - name: COGNITIVE_TIMEOUT_S
                  value: "8.0"
                - name: COGNITIVE_MAX_INFLIGHT
                  value: "64"
                - name: FAST_PATH_LATENCY_SLO_MS
                  value: "1000"
                - name: MAX_PLAN_STEPS
                  value: "16"
                - name: DSP_LOG_TO_FILE
                  value: "false"
                - name: DSP_LOG_TO_STDOUT
                  value: "true"
                - name: DSP_LOG_PATH
                  value: "/tmp/azure_openai_usage.log"
                - name: LOG_TO_FILE
                  value: "false"
                - name: LOG_TO_STDOUT
                  value: "true"
                - name: LOG_LEVEL
                  value: "INFO"
              ports:
                - name: gcs
                  containerPort: 6379
                - name: dashboard
                  containerPort: 8265
                - name: client
                  containerPort: 10001
                # ✅ FIX: Updated the containerPort to match the new httpOptions port.
                - name: serve
                  containerPort: 8000
                - name: metrics
                  containerPort: 8080
              resources:
                requests:
                  cpu: "500m"
                  memory: "4Gi"
                limits:
                  cpu: "2"
                  memory: "8Gi"
              volumeMounts:
                - name: xgb-model-storage
                  mountPath: /app/data
                - name: project-src
                  mountPath: /app
              # Probes are good practice for production readiness.
              startupProbe:
                exec:
                  command:
                    [
                      "bash",
                      "-lc",
                      "wget --tries=1 -T 2 -q -O- http://127.0.0.1:52365/api/local_raylet_healthz | grep -q success",
                    ]
                periodSeconds: 5
                timeoutSeconds: 3
                failureThreshold: 72
              readinessProbe:
                exec:
                  command:
                    [
                      "bash",
                      "-lc",
                      "wget --tries=1 -T 2 -q -O- http://127.0.0.1:52365/api/local_raylet_healthz | grep -q success",
                    ]
                periodSeconds: 5
                timeoutSeconds: 2
                failureThreshold: 3
              livenessProbe:
                exec:
                  command:
                    [
                      "bash",
                      "-lc",
                      "wget --tries=1 -T 2 -q -O- http://127.0.0.1:52365/api/local_raylet_healthz | grep -q success",
                    ]
                periodSeconds: 5
                timeoutSeconds: 2
                failureThreshold: 120
          volumes:
            - name: xgb-model-storage
              persistentVolumeClaim:
                claimName: xgb-pvc
            - name: project-src
              hostPath:
                path: /project
                type: Directory

    # -------------------------------------------------------------------
    #  Worker Node Specification
    # -------------------------------------------------------------------
    workerGroupSpecs:
      - groupName: small
        replicas: 1
        rayStartParams:
          num-cpus: "2"
        template:
          spec:
            containers:
              - name: ray-worker
                image: seedcore:latest
                imagePullPolicy: IfNotPresent
                envFrom:
                  - secretRef: { name: seedcore-env-secret }
                env:
                  - name: POD_NAMESPACE
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.namespace
                  - name: SERVE_SERVICE
                    value: "stable"
                  - name: DGL_GRAPHBOLT_SKIP
                    value: "0"
                  - name: RAY_NAMESPACE
                    value: "seedcore-dev"
                  - name: SEEDCORE_NS
                    value: "seedcore-dev"
                  - name: XGB_STORAGE_PATH
                    value: "/app/data/models"
                  # ✅ Routing environment variables for coordinator access
                  - name: OCPS_DRIFT_THRESHOLD
                    value: "0.5"
                  - name: COGNITIVE_TIMEOUT_S
                    value: "8.0"
                  - name: COGNITIVE_MAX_INFLIGHT
                    value: "64"
                  - name: FAST_PATH_LATENCY_SLO_MS
                    value: "1000"
                  - name: MAX_PLAN_STEPS
                    value: "16"
                  - name: DSP_LOG_TO_FILE
                    value: "false"
                  - name: DSP_LOG_TO_STDOUT
                    value: "true"
                  - name: DSP_LOG_PATH
                    value: "/tmp/azure_openai_usage.log"
                  - name: LOG_TO_FILE
                    value: "false"
                  - name: LOG_TO_STDOUT
                    value: "true"
                  - name: LOG_LEVEL
                    value: "INFO"
                resources:
                  requests:
                    cpu: "500m"
                    memory: "4Gi"
                  limits:
                    cpu: "2"
                    memory: "8Gi"
                volumeMounts:
                  - name: xgb-model-storage
                    mountPath: /app/data
                  - name: project-src
                    mountPath: /app
                readinessProbe:
                  exec:
                    command:
                      [
                        "bash",
                        "-lc",
                        "wget --tries=1 -T 2 -q -O- http://127.0.0.1:52365/api/local_raylet_healthz | grep -q success",
                      ]
                  periodSeconds: 5
                  timeoutSeconds: 2
                  failureThreshold: 3
                livenessProbe:
                  exec:
                    command:
                      [
                        "bash",
                        "-lc",
                        "wget --tries=1 -T 2 -q -O- http://127.0.0.1:52365/api/local_raylet_healthz | grep -q success",
                      ]
                  periodSeconds: 5
                  timeoutSeconds: 2
                  failureThreshold: 120
            volumes:
              - name: xgb-model-storage
                persistentVolumeClaim:
                  claimName: xgb-pvc
              - name: project-src
                hostPath:
                  path: /project
                  type: Directory
