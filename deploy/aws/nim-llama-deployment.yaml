apiVersion: apps/v1
kind: Deployment
metadata:
  name: nim-llama
  namespace: ${NAMESPACE}
  labels:
    app: nim-llama
    component: nim
    version: v1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nim-llama
  template:
    metadata:
      labels:
        app: nim-llama
        component: nim
        version: v1
    spec:
      serviceAccountName: seedcore-service-account
      containers:
        - name: nim-llama
          image: ${ECR_LLAMA_REPO}:${NIM_LLAMA_TAG}
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
            - name: metrics
              containerPort: 9090
              protocol: TCP
          env:
            # NIM Configuration
            - name: NIM_MODEL_NAME
              value: "llama-model"
            - name: NIM_PORT
              value: "8000"
            - name: NIM_HOST
              value: "0.0.0.0"
            
            # AWS Configuration
            - name: AWS_REGION
              value: "${AWS_REGION}"
            - name: AWS_DEFAULT_REGION
              value: "${AWS_REGION}"
            
            # Logging Configuration
            - name: LOG_LEVEL
              value: "INFO"
            - name: LOG_FORMAT
              value: "json"
            
            # Performance Configuration
            - name: WORKERS
              value: "1"
            - name: MAX_CONCURRENT_REQUESTS
              value: "50"
            
            # Model Configuration
            - name: MODEL_CACHE_DIR
              value: "/app/models"
            - name: MAX_TOKENS
              value: "2048"
            - name: TEMPERATURE
              value: "0.7"
            - name: TOP_P
              value: "0.9"
            
            # GPU Configuration
            - name: CUDA_VISIBLE_DEVICES
              value: "0"
            - name: NVIDIA_VISIBLE_DEVICES
              value: "0"
          
          envFrom:
            - configMapRef:
                name: nim-llama-config
                optional: true
            - secretRef:
                name: nim-llama-secret
                optional: true
          
          volumeMounts:
            - name: model-storage
              mountPath: /app/models
            - name: cache-storage
              mountPath: /app/cache
            - name: logs-volume
              mountPath: /app/logs
            - name: config-volume
              mountPath: /app/config
              readOnly: true
          
          resources:
            requests:
              memory: "8Gi"
              cpu: "2000m"
              nvidia.com/gpu: 1
            limits:
              memory: "16Gi"
              cpu: "4000m"
              nvidia.com/gpu: 1
          
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 15
            timeoutSeconds: 10
            failureThreshold: 5
          
          startupProbe:
            httpGet:
              path: /health
              port: 8000
            failureThreshold: 120
            periodSeconds: 10
          
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 120
            periodSeconds: 30
            timeoutSeconds: 15
            failureThreshold: 3
          
          # Security context
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: false
            capabilities:
              drop:
                - ALL
      
      volumes:
        - name: model-storage
          persistentVolumeClaim:
            claimName: nim-llama-model-pvc
        - name: cache-storage
          emptyDir: {}
        - name: logs-volume
          emptyDir: {}
        - name: config-volume
          configMap:
            name: nim-llama-config
            optional: true
      
      # Node selection for GPU nodes
      nodeSelector:
        node-type: gpu
      
      # Tolerations for GPU nodes
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      
      # Pod disruption budget
      terminationGracePeriodSeconds: 60
      
      # Security context
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000

---
apiVersion: v1
kind: Service
metadata:
  name: nim-llama
  namespace: ${NAMESPACE}
  labels:
    app: nim-llama
    component: nim
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
spec:
  type: ClusterIP
  selector:
    app: nim-llama
  ports:
    - name: http
      port: 8000
      targetPort: 8000
      protocol: TCP
    - name: metrics
      port: 9090
      targetPort: 9090
      protocol: TCP
  sessionAffinity: None

---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: nim-llama-pdb
  namespace: ${NAMESPACE}
spec:
  minAvailable: 0
  selector:
    matchLabels:
      app: nim-llama


---
apiVersion: v1
kind: ConfigMap
metadata:
  name: nim-llama-config
  namespace: ${NAMESPACE}
  labels:
    app: nim-llama
data:
  config.yaml: |
    model:
      name: "llama-model"
      version: "latest"
      cache_dir: "/app/models"
    
    server:
      host: "0.0.0.0"
      port: 8000
      workers: 1
      max_concurrent_requests: 50
    
    generation:
      max_tokens: 2048
      temperature: 0.7
      top_p: 0.9
      repetition_penalty: 1.1
    
    performance:
      batch_size: 1
      timeout: 60
      retry_attempts: 2
