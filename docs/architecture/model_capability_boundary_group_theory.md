# Model Capability Boundary via Group-Theory-Embedded Solutions

This document describes the architecture for evaluating model capability boundaries using group-theory-embedded solutions. The pipeline transforms text embeddings through dimensionality reduction, Lie group mapping, and tangent-space statistical analysis to produce probabilistic semantic scores.

## Overview

The capability boundary evaluation pipeline consists of seven main stages:

1. **Text Embedding** - Transform text to high-dimensional vectors
2. **Dimensionality Reduction** - Reduce to k-dimensional space via PCA
3. **Lie Algebra Mapping** - Map vectors to skew-symmetric matrices
4. **Lie Group Mapping** - Exponentiate to rotation group elements
5. **Tangent Space Projection** - Project to Euclidean tangent space
6. **Tangent-Space Gaussian Model** - Compute covariance and Mahalanobis distance
7. **Probabilistic Scoring** - Convert distance to semantic score

## Architecture

### Full Pipeline Diagram: From Embedding â†’ Lie Group â†’ Tangent-Space Mahalanobis

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  1. Text Embedding Stage                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  |  (SentenceTransformer / NV-Embed)
                  v
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  High-dim Vector x  â”‚  â† in R^d  (e.g., d=4096 or 1024)
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  |
                  |
                  v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 2. Dimensionality Reduction               â”‚
â”‚        (scikit-learn â†’ PCA / Robust Scaling / Whitening)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  |
                  v
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  PCA Vector  z      â”‚  â† in R^k   (k = m(m-1)/2 )
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  |
                  |
                  v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  3. Lie Algebra Mapping                   â”‚
â”‚      (Geomstats OR SciPy: create Î¾ âˆˆ so(m) from z vector) â”‚
â”‚      Uses: reshape â†’ skew-symmetric matrix construction   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  |
                  v
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Lie Algebra Î¾      â”‚  â† element of ð”°ð”¬(m)
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  |
                  |  SciPy: expm(Î¾)
                  v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  4. Lie Group Mapping                     â”‚
â”‚            (Geomstats or SciPy: g = expm(Î¾))              â”‚
â”‚                      g âˆˆ SO(m)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  |
                  |
                  v
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Group Element g   â”‚  â† rotation-like structure
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  |
                  |  Compare to mean group element g_mean
                  v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            5. Tangent Space Projection                    â”‚
â”‚   (Geomstats: logarithmic map log_{g_mean}(g) )           â”‚
â”‚   (SciPy: logm(g_mean^{-1} Â· g))                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  |
                  v
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Tangent Vector  v   â”‚  â† in R^k, local Euclidean
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  |
                  |  SciPy / sklearn: compute covariance
                  v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             6. Tangent-Space Gaussian Model               â”‚
â”‚       (scikit-learn + SciPy: covariance, Î¼=0, Î£_g)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  |
                  | Compute Mahalanobis DÂ² = váµ€ Î£â»Â¹ v
                  v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   7. Convert Distance â†’ Soft Probabilistic Score          â”‚
â”‚   (SciPy: score = 1 - Ï‡Â²_cdf(DÂ², df=k))                   â”‚
â”‚   Interpretation: "How normal is the answer?"             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  |
                  v
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Semantic Score [0,1]â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Compact Data-Flow Map

```
Text
 â†’ Embedding (Transformer)
 â†’ PCA (sklearn)
 â†’ z âˆˆ R^k
 â†’ Î¾ = vecâ†’skew (Geomstats or SciPy)
 â†’ g = exp(Î¾)  (SciPy linalg.expm)
 â†’ v = log(g_mean^{-1} g)  (SciPy logm)
 â†’ Mahalanobis in tangent space (SciPy + sklearn)
 â†’ score = 1 - Ï‡Â²_cdf(DÂ²)
```

## Technology Breakdown per Stage

### ðŸ“˜ Stage 1: Embedding

**Libraries**: SentenceTransformer, NV-Embed

**Purpose**: Transform text input to high-dimensional vector representations

**Output**: Vector `x âˆˆ R^d` where `d` is typically 4096 or 1024

**Note**: This stage does not use sklearn/SciPy/Geomstats

---

### ðŸ“™ Stage 2: Dimensionality Reduction â†’ scikit-learn

**Libraries**: scikit-learn

**Key Components**:
- `PCA` - Principal Component Analysis
- `StandardScaler`, `RobustScaler` - Feature scaling
- `Covariance`, `EllipticEnvelope` (optional) - Covariance estimation

**Purpose**: Reduce high-dimensional embeddings to k-dimensional space where `k = m(m-1)/2`

**Output**: PCA vector `z âˆˆ R^k`

**Mathematical Foundation**: The choice of `k = m(m-1)/2` is criticalâ€”it represents the dimension of the `so(m)` Lie algebra, creating a 1-to-1 isomorphism between the PCA space and the Lie algebra tangent space.

---

### ðŸ“— Stage 3-5: Lie Groups â†’ Geomstats

**Libraries**: Geomstats (primary), SciPy (supporting)

**Key Components**:
- `geomstats.geometry.special_orthogonal.SpecialOrthogonal` - SO(m) group structure
- `exp`, `log` - Group exponential and logarithmic maps
- Riemannian metrics - Geodesic distances
- Karcher mean - FrÃ©chet mean on the manifold

**Purpose**: 
- Map PCA vectors to Lie algebra elements `Î¾ âˆˆ so(m)`
- Exponentiate to group elements `g âˆˆ SO(m)`
- Compute group mean `g_mean` using Karcher mean

**Why Geomstats**: Geomstats is specifically built for:
- Lie groups and Riemannian manifolds
- Tangent-space geometry
- Geodesic distances
- Karcher mean computation (iterative optimization)

**Alternative**: While SciPy can perform `expm` and `logm`, Geomstats provides optimized, validated implementations for manifold statistics.

---

### ðŸ“˜ Stage 4 & 7: Linear Algebra & Statistics â†’ SciPy

**Libraries**: SciPy

**Critical Operations**:

| Operation            | SciPy Function         | Purpose                          |
| -------------------- | ---------------------- | -------------------------------- |
| Matrix exponential   | `scipy.linalg.expm`    | Lie algebra â†’ Lie group          |
| Matrix logarithm     | `scipy.linalg.logm`    | Lie group â†’ tangent space        |
| Covariance inversion | `scipy.linalg.inv`     | Mahalanobis distance computation |
| Solving systems      | `scipy.linalg.solve`   | Efficient linear algebra         |
| Chi-square CDF       | `scipy.stats.chi2.cdf` | Distance â†’ probability           |

**Purpose**: Provide the underlying mathematical engine for:
- Lie algebra exponentials/logs
- Mahalanobis distance computation
- Statistical normality tests

**Why SciPy**: `scipy.linalg.expm` and `scipy.linalg.logm` are gold-standard, numerically stable implementations. `scipy.stats.chi2` provides statistically correct conversion from Mahalanobis distance to probability.

---

### ðŸ“™ Stage 6: Tangent-Space Statistics â†’ scikit-learn + SciPy

**Libraries**: scikit-learn, SciPy

**Key Insight**: Once projected to the tangent space at `g_mean`, we are in a **standard Euclidean vector space**. All standard Gaussian statistics are valid.

**Components**:
- Covariance estimation (scikit-learn)
- Mahalanobis distance: `DÂ² = váµ€ Î£â»Â¹ v` (SciPy)
- Mean vector: `Î¼ = 0` (by construction, since we're at the mean)

**Output**: Mahalanobis distance `DÂ²` representing deviation from the mean capability distribution

---

### ðŸ“˜ Stage 7: Probabilistic Scoring â†’ SciPy

**Libraries**: SciPy

**Transformation**: 
```
score = 1 - Ï‡Â²_cdf(DÂ², df=k)
```

**Interpretation**: 
- `score âˆˆ [0, 1]` represents "how normal is the answer?"
- Higher scores indicate answers closer to the mean capability distribution
- Lower scores indicate outliers or capability boundary violations

**Statistical Foundation**: The Mahalanobis distance `DÂ²` follows a chi-square distribution with `k` degrees of freedom under the null hypothesis of normal distribution.

## Implementation Details

### Key Mathematical Properties

#### 1. Dimensionality Isomorphism

The critical insight is the isomorphism:
```
R^k â‰… so(m)  where k = m(m-1)/2
```

This creates a 1-to-1 mapping between:
- PCA-reduced embedding space (`R^k`)
- Lie algebra tangent space (`so(m)`)

**Example**: For `m = 16`, we have `k = 16 Ã— 15 / 2 = 120` dimensions.

#### 2. Tangent Space Euclidean Structure

After projection to the tangent space at `g_mean`, we operate in a **standard Euclidean vector space**. This enables:
- Standard Gaussian statistics (covariance, Mahalanobis distance)
- Linear operations
- Classical statistical tests

#### 3. Statistical Validity

The Mahalanobis distance `DÂ² = váµ€ Î£â»Â¹ v` follows a chi-square distribution:
```
DÂ² ~ Ï‡Â²(k)
```

This provides a principled way to convert distances to probabilities.

### Library Responsibility Map

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Text Input                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  SentenceTransformer/NV-Embed  â”‚  â† External
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      scikit-learn (PCA)        â”‚  â† Dimensionality Reduction
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Geomstats (Lie Groups)       â”‚  â† Manifold Operations
        â”‚   SciPy (expm/logm)           â”‚  â† Matrix Operations
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ scikit-learn (Covariance)      â”‚  â† Statistics
        â”‚ SciPy (Mahalanobis, Ï‡Â²)        â”‚  â† Distance & Probability
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      Semantic Score [0,1]      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Architecture Analysis

### Why This Architecture is Excellent

#### 1. Correct Separation of Concerns

- **scikit-learn**: Used for its strengthâ€”data preprocessing and linear dimensionality reduction (PCA)
- **Geomstats**: Used for its specific, advanced strengthâ€”handling the geometry, metrics, and statistics (Karcher mean) of the `SO(m)` manifold
- **SciPy**: Used for its low-level, high-performance mathematical engine (`expm`, `logm`, `chi2`) that Geomstats and custom code build upon

#### 2. Mathematical Correctness

The entire plan hinges on the `R^k` vector space (from PCA) being **isomorphic** to the `so(m)` Lie algebra tangent space. The choice of `k = m(m-1)/2` (e.g., 120 dims for m=16) is the precise mathematical link that makes this architecture "click." It's not a heuristic; it's a 1-to-1 mapping.

#### 3. Statistical Soundness

The most critical step is **Tangent Space Projection**. Once projected to the tangent space at the mean (`g_mean`), we are in a **standard Euclidean vector space**. Therefore, all standard Gaussian statistics (covariance, Mahalanobis distance) are 100% valid. This is the correct way to "do stats on a manifold."

### Key Implementation Endorsements

#### Geomstats for Manifold Operations

While you *can* build the Karcher mean logic with just `scipy.linalg`, it's an iterative optimization process. Geomstats has this logic built-in, validated, and optimized.

**Use Cases**:
- Karcher mean computation (FrÃ©chet mean on SO(m))
- Geodesic distance calculations
- Group exponential/logarithmic maps
- Riemannian metric computations

#### SciPy for Core Mathematical Operations

`scipy.linalg.expm` and `scipy.linalg.logm` are the gold-standard, numerically stable implementations. `scipy.stats.chi2` is the statistically correct way to convert your `DÂ²` distance (a sum of squared Gaussian-like variables) into a probability-based score.

**Use Cases**:
- Matrix exponential: `g = expm(Î¾)`
- Matrix logarithm: `v = logm(g_mean^{-1} Â· g)`
- Covariance inversion: `Î£â»Â¹`
- Chi-square CDF: `score = 1 - Ï‡Â²_cdf(DÂ², df=k)`

### Overall Verdict

This is a **production-ready, "S-tier" design**. It's not a "magical" black box but a series of well-defined, classical mathematical and statistical transformations.

The pipeline is perfectly specified. The technology choices are exactly what a specialist in geometric data analysis would select.

## Implementation Considerations

### Performance Characteristics

#### Computational Complexity

- **Embedding**: O(d) where d is embedding dimension
- **PCA**: O(dÂ²) for covariance, O(dk) for projection
- **Lie Group Operations**: O(mÂ³) for matrix exponential/logarithm
- **Tangent Space Statistics**: O(kÂ²) for covariance, O(kÂ³) for inversion
- **Overall**: Dominated by O(mÂ³) operations for typical m=16

#### Memory Requirements

- **Embeddings**: O(N Ã— d) for N samples
- **PCA Model**: O(d Ã— k) for transformation matrix
- **Covariance Matrix**: O(kÂ²) for tangent space
- **Group Elements**: O(N Ã— mÂ²) for N samples

### Scalability Considerations

1. **Batch Processing**: Process multiple embeddings in batches for efficiency
2. **Caching**: Cache PCA model and group mean after training
3. **Incremental Updates**: Update covariance incrementally for streaming data
4. **Dimensionality Selection**: Choose `m` based on computational budget (m=16 â†’ k=120 is a good balance)

### Error Handling

1. **Numerical Stability**: Use SciPy's numerically stable `expm`/`logm`
2. **Singular Covariance**: Regularize covariance matrix if singular
3. **Out-of-Distribution**: Handle cases where tangent projection fails
4. **Embedding Failures**: Graceful fallback for embedding errors

## Configuration

### Key Parameters

```python
# Dimensionality
m = 16                    # SO(m) group dimension
k = m * (m - 1) // 2     # Lie algebra dimension (120 for m=16)
d = 4096                  # Embedding dimension

# PCA Configuration
n_components = k          # Match Lie algebra dimension
whiten = True             # Optional whitening

# Statistical Parameters
regularization = 1e-6     # Covariance regularization
confidence_level = 0.95   # Chi-square confidence level
```

### Library Versions

- **scikit-learn**: >= 1.0.0 (for PCA and preprocessing)
- **Geomstats**: >= 2.5.0 (for Lie group operations)
- **SciPy**: >= 1.7.0 (for matrix operations and statistics)
- **SentenceTransformer**: >= 2.0.0 (for embeddings)

## Future Enhancements

### Research Directions

1. **Adaptive Dimensionality**: Dynamically select `m` based on data characteristics
2. **Non-Euclidean Metrics**: Explore alternative Riemannian metrics beyond standard
3. **Multi-Manifold Models**: Combine multiple Lie groups for richer representations
4. **Online Learning**: Incremental updates to group mean and covariance

### Performance Optimizations

1. **GPU Acceleration**: Leverage GPU for matrix operations (cuPy)
2. **Approximate Methods**: Use approximate Karcher mean for faster computation
3. **Sparse Representations**: Exploit sparsity in covariance matrices
4. **Parallel Processing**: Parallelize across multiple samples

---

*This document provides a comprehensive guide to the Model Capability Boundary evaluation architecture using group-theory-embedded solutions. The design is mathematically rigorous, statistically sound, and production-ready.*

